from turtle import forward
from typing import List
from contextlib import asynccontextmanager
from datetime import datetime
import json
import os
from fastapi.routing import APIRoute
from pydantic import BaseModel
from regex import P
import torch
import faiss
import pickle
import gzip
import numpy as np
from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel
from fastapi import FastAPI, Query
import logging
import uvicorn
import pandas as pd
import joblib
from pathlib import Path
from typing import Optional
import numpy as np
import torch.nn as nn
from typing import List
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage
from fastapi.responses import JSONResponse

#----------------------------------- ì„œë²„ ì‹œì‘ì „ ë¡œë”©ë˜ëŠ” ì½”ë“œ
#-----------------------------------
#-----------------------------------
#-----------------------------------
#-----------------------------------

# í™˜ê²½ë³€ìˆ˜ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()


# ë°ì´í„° ê¸°ë³¸ê²½ë¡œ
DATA_DIR = "./data"
MODEL_DIR = "./model"
# ì„œë²„ ë””ë²„ê±°
logging.basicConfig(
    level=logging.INFO,  
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

# ë¡œë“œí• íŒŒì¼ê²½ë¡œ
FAISS_INDEX_PATH = os.path.join(DATA_DIR, "faiss_index.bin")
STORE_NAMES_PATH = os.path.join(DATA_DIR, "store_names.pkl.gz")
CATEGORY_PATH = os.path.join(DATA_DIR, "category_data.pkl.gz")

# ë°ì´í„° ë¯¸ë¦¬ ìƒì„±(ë¹„ë™ê¸°ì²˜ë¦¬)
index = None
store_names = []
category_dict = {}

# ëª¨ë¸ë¡œë“œ
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1', tokenizer_class="KoBERTTokenizer")
model = BertModel.from_pretrained('skt/kobert-base-v1')
kmeans_model = joblib.load("./model/kmeans_model.pkl")
model.eval()


@asynccontextmanager
async def load_dataset(app : FastAPI):
    global index, store_names, category_dict
    
    # ì¸ë±ìŠ¤ë¡œë”©
    index = faiss.read_index(FAISS_INDEX_PATH)
    logger.info("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
    # ê°€ê²Œì´ë¦„ë¡œë”©
    with gzip.open(STORE_NAMES_PATH, "rb") as f:
        store_names = pickle.load(f)
    logger.info(f"âœ… ì €ì¥ëœ ê°€ê²Œ ê°œìˆ˜: {len(store_names)}ê°œ")    

    # ê°€ê²Œ ì¹´í…Œê³ ë¦¬ ë¡œë”©
    with gzip.open(CATEGORY_PATH, "rb") as f:
        category_dict = pickle.load(f)
    logger.info(f"âœ… ì €ì¥ëœ ì¹´í…Œê³ ë¦¬ ê°œìˆ˜: {len(category_dict)}ê°œ")
    yield
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì‹¤í–‰í•  ì½”ë“œ
    print("ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
# ì„ë² ë”© ìƒì„±í•˜ê¸°(ë‚´ í…ìŠ¤íŠ¸ì˜)
def get_kobert_embeddings(texts):
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=64)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()
app = FastAPI(lifespan=load_dataset)

#----------------------------------- ê¸°ë³¸ì ì¸ API ì‹œì‘í•˜ëŠ” ë¶€ë¶„
#-----------------------------------
#-----------------------------------
#-----------------------------------
#-----------------------------------
#-----------------------------------
#-----------------------------------
#-----------------------------------
# ê²€ìƒ‰í•˜ëŠ” api
@app.get("/search")
async def search_store(query : str = Query(..., description="ê²€ìƒ‰í•  ë§¤ì¥ ì´ë¦„")):
    # 1. ì„ë² ë”© ìƒì„±í•˜ê¸°
    query_embedding = get_kobert_embeddings([query])
    
    # 2. ê²°ê³¼ë¥¼ ì •ê·œí™”í•˜ê¸°
    query_embedding_normalization = query_embedding / np.linalg.norm(query_embedding)
    
    # 3. FAISS ê²€ìƒ‰ì„¤ì •
    k = 1 # í•œê°œë§Œë°˜í™˜
    distance, indice = index.search(query_embedding, k)
    
    # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ì˜ ê²°ê³¼ ë°˜í™˜
    best_idx = indice[0][0]
    best_store_name = store_names[best_idx]
    best_category = category_dict.get(best_store_name, "None")
    best_score  = float(distance[0][0])
    
    return {
    "query": query,
    "best_match": {
        "store_name": best_store_name,
        "category": best_category,
        "score": best_score
        }
    }
    
    
#-----------------------------------
#-----------------------------------
#-----------------------------------
# ì†Œë¹„ìŠµê´€ê³¼ ì—°ë ¹ëŒ€ ë¶„ì„í•˜ëŠ” API
# ì†Œë¹„ìŠµê´€ Springì—°ë™ì„ ìœ„í•œ Base Data êµ¬ì„±í•˜ê¸°

# ë¦¬ìŠ¤íŠ¸ì•ˆì—ë“¤ì–´ìˆëŠ” ë°ì´í„°ì˜ í˜•ì‹
class Transaction(BaseModel):
    id : int
    amount: int
    createdAt: datetime
    category: str
    item: str

# Jsonë°ì´í„°ì˜ í˜•ì‹
class ConsumptionList(BaseModel):
    transactions : List[Transaction]

# # í…ŒìŠ¤íŠ¸ë°ì´í„°
# ì–˜ í˜•ì‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í–ˆì„ë•Œ ì„±ê³µí•¨
# consumptionë°ì´í„° ì°¸ê³  ê³„ì¢Œê´€ë ¨ ë‹¤ 0ìœ¼ë¡œì²˜ë¦¬                                                                                                   
# json_path = Path("C:/Users/SSAFY/Downloads/consumption_3months.json")
# with open(json_path, "r", encoding="utf-8") as f:
#     json_data = json.load(f)
# data = ConsumptionList(**json_data)

##################################
##################################
################## ëª¨ë¸ ì „ì²˜ë¦¬ì™€ ì¹´í…Œê³ ë¦¬ ë°ì´í„°
label_list = ["ì²­ë…„ì¸µ", "ì¤‘ë…„ì¸µ", "ì¤‘ì¥ë…„ì¸µ", "ì¥ë…„ì¸µ", "ë…¸ë…„ì¸µ"]
kmeans_model = joblib.load("./model/kmeans_model.pkl")
tag_df = pd.read_csv("./data/tag_ratio_df.csv")
tag_df = tag_df.rename(columns={"Unnamed: 0": "age_group"})
tag_df = tag_df.set_index("age_group")

class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        out, _ = self.gru(x)
        out = self.fc(out[:, -1, :])
        return out
    
from decimal import Decimal, ROUND_HALF_UP

def to_decimal(v):
    return Decimal(str(v)).quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)

#-----------------------------------
#-----------------------------------
#-----------------------------------
# ë¶„ì„ ì „ì²˜ë¦¬ ë° ì‹œì‘ì 
@app.post("/sobi-analyze")
async def sobi_analyze(data : ConsumptionList):
    df = pd.DataFrame([t.model_dump() for t in data.transactions])
    print(df)
    df["createdAt"] = pd.to_datetime(df["createdAt"]) # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ í…ŒìŠ¤íŠ¸
    # GRUëª¨ë¸ì„ ìœ„í•œ 3ë‹¬ì§œë¦¬ ë°ì´í„°í”„ë ˆì„
    df["year_month"] = df["createdAt"].dt.to_period("M")

    # pivot í…Œì´ë¸” ìƒì„±
    gru_pivot = df.pivot_table(
        index="year_month",
        columns="category",
        values="amount",
        aggfunc="sum"
    ).fillna(0)
    
    gru_percent = gru_pivot.div(gru_pivot.sum(axis=1), axis=0) * 100
    gru_percent = gru_percent.fillna(0).round(2)

    # ì¹´í…Œê³ ë¦¬ ìˆœì„œê³ ì •
    CATEGORY_ORDER = ["ìŒì‹", "ê³¼í•™Â·ê¸°ìˆ ", "ì†Œë§¤", "ë¶€ë™ì‚°", "ì˜ˆìˆ Â·ìŠ¤í¬ì¸ ",
                    "ìˆ™ë°•", "ìˆ˜ë¦¬Â·ê°œì¸", "ì‹œì„¤ê´€ë¦¬Â·ì„ëŒ€", "êµìœ¡", "ë³´ê±´ì˜ë£Œ"]

    # fillna(0) ì´ì§€ë§Œ í•œë²ˆë” ì²´í¬
    for col_name in CATEGORY_ORDER:
        if col_name not in gru_percent.columns:
            gru_percent[col_name] = 0
            
    gru_percent = gru_percent[CATEGORY_ORDER] # ì»¬ëŸ¼ìˆœì„œ í•­ìƒ ì¼ì¹˜í•˜ë„ë¡ ì¡°ì •
    
    # 3ê°œì›” ë°ì´í„° ì´í•˜ì¼ê²½ìš°? ë¶„ì„ x
    if len(gru_percent) < 3:
        return {"error" : "3ê°œì›” ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}
    
    kmeans_data = df.groupby("category")["amount"].sum()
    vector = kmeans_data.reindex(CATEGORY_ORDER).fillna(0)
    vector = vector / vector.sum() * 100
    
    cluster_age = kmeans_model.predict([vector])[0]
    age_text = label_list[cluster_age]
    age_group_name = tag_df.index[cluster_age]
    age_vector = tag_df.loc[age_group_name]
    diff = (vector - age_vector).round(2)
    result1 = {"style" : f"ë¶„ì„ê²°ê³¼ {age_text}ì†Œë¹„ íŒ¨í„´ì— ë¹„êµì  ê°€ê¹Œì›Œìš”.", "analyze_result": []}

    for cat, delta in diff.items():
        direction = "ì¦ê°€ ì˜ˆì •" if delta > 0 else "ê°ì†Œ ì˜ˆì •"
        result1["analyze_result"].append({
            "category": cat,
            "diff": round(float(delta), 2),
            "direction": direction
        })
    
    input_size = gru_percent.shape[1]   # ë³´í†µ 10ê°œ
    hidden_size = 32
    output_size = input_size

    model = GRUModel(input_size, hidden_size, output_size)
    model_path = r"./model/best_gru_model.pt"
    model.load_state_dict(torch.load(model_path))
    model.eval()
    X_input = gru_percent.values[-3:]  # shape: (3, 10)
    X_tensor = torch.tensor(X_input[np.newaxis, :, :], dtype=torch.float32)  # shape: (1, 3, 10)
    
    with torch.no_grad():
        prediction = model(X_tensor).numpy()
    
    CATEGORY_ORDER = gru_percent.columns.tolist()
    # 1. ì˜ˆì¸¡ ê²°ê³¼ (ì†Œìˆ˜ì  2ìë¦¬ ë°˜ì˜¬ë¦¼)
    predicted_result = {
        k: to_decimal(v)
        for k, v in zip(CATEGORY_ORDER, prediction[0])
    }
    # 2. ìµœê·¼ ì›”ì˜ ì‹¤ì œ ì†Œë¹„ ë¹„ìœ¨
    original_result = {
        k: to_decimal(v)
        for k, v in gru_percent.iloc[-1].items()
    }

    # 3. ë³€í™”ëŸ‰ ê³„ì‚° (ì˜ˆì¸¡ê°’ - ì‹¤ì œê°’)
    diff_result = {
        k: to_decimal(predicted_result[k] - original_result[k])
        for k in CATEGORY_ORDER
    }
    
    res_list = []
    for category in CATEGORY_ORDER:
        before = original_result[category]
        after = predicted_result[category]
        delta = diff_result[category]
        direction = "ë” ì†Œë¹„" if delta > 0 else "ëœ ì†Œë¹„" if delta < 0 else "ë³€í™” ì—†ìŒ"

        res_list.append({
            "category": category,
            "ì›ë˜ëŸ‰": before,
            "ì˜ˆì¸¡ê°’": after,
            "ë³€í™”ëŸ‰": delta,
            "í•´ì„": direction
        })

    # âœ… ìµœì¢… ë°˜í™˜ êµ¬ì¡°
    # springì—ì„œëŠ” doubleë¡œ ë°›ìœ¼ì‡¼ ë’¤ì§€ê¸°ì‹«ìœ¼ë©´(DTO)
    return {
        "ìš”ì•½": result1["style"],
        "result": res_list
    }



#-----------------------------------
#-----------------------------------
#-----------------------------------
# ë¶„ì„ ì „ì²˜ë¦¬ ë° ì‹œì‘ì 
@app.post("/card-recommend")
async def card_recommend(data: ConsumptionList):
    # ë¶„ì„ ìœ„ì— ì²˜ëŸ¼ ë°ì´í„° í”„ë ˆì„ ë°›ì•„ì™€ì„œ êµ¬ì„±í•˜ê¸°
    # pivot tableì„ í†µí•´ ìš”ì•½
    df = pd.DataFrame([t.model_dump() for t in data.transactions])
    df["createdAt"] = pd.to_datetime(df["createdAt"])
    df["year_month"] = df["createdAt"].dt.to_period("M")
    llm_pivot = df.pivot_table(index="year_month", columns="category", values="amount", aggfunc="sum").fillna(0)
    percent = llm_pivot.div(llm_pivot.sum(axis=1), axis=0) * 100
    percent = percent.fillna(0).round(2)
    
    CATEGORY_ORDER = ["ìŒì‹", "ê³¼í•™Â·ê¸°ìˆ ", "ì†Œë§¤", "ë¶€ë™ì‚°", "ì˜ˆìˆ Â·ìŠ¤í¬ì¸ ",
                "ìˆ™ë°•", "ìˆ˜ë¦¬Â·ê°œì¸", "ì‹œì„¤ê´€ë¦¬Â·ì„ëŒ€", "êµìœ¡", "ë³´ê±´ì˜ë£Œ"]

    lim_data = df.groupby("category")["amount"].sum()
    vector = lim_data.reindex(CATEGORY_ORDER).fillna(0)
    vector = vector / vector.sum() * 100
    
    # ìœ ì € ì„±í–¥ ê°„ë‹¨ ìš”ì•½
    user_profile = ", ".join([f"{cat} {val:.1f}%" for cat, val in vector.items() if val > 0])
    
    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    credits_data = pd.read_json("./data/credit_cards_cleaned_raw.json")
    check_data = pd.read_json("./data/check_cards_cleaned_raw.json")
    # ê°ê° 20ê°œì”© ëœë¤ ìƒ˜í”Œë§
    credit_sample = credits_data.sample(n=20, random_state=42)
    check_sample = check_data.sample(n=20, random_state=42)
    api_key = os.getenv("OPENAI_API_KEY")
    # í•©ì¹˜ê¸°
    all_cards = pd.concat([credit_sample, check_sample]).reset_index(drop=True)
    prompt = PromptTemplate(
        input_variables=["user_profile", "card_list"],
        template="""
        ë‹¹ì‹ ì€ ì¹´ë“œ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì†Œë¹„ ì„±í–¥ì…ë‹ˆë‹¤:

        {user_profile}

        ë‹¤ìŒì€ ì¶”ì²œ ê°€ëŠ¥í•œ ì¹´ë“œ ëª©ë¡ì…ë‹ˆë‹¤:

        {all_cards}

        ì‚¬ìš©ìì—ê²Œ ì í•©í•œ ì¹´ë“œ 3ê°œë¥¼ ì¶”ì²œí•˜ê³ , ê° ì¹´ë“œì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        
        ë‹¨ ì‹ ìš©ì¹´ë“œ, ì²´í¬ì¹´ë“œ ë³„ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…í•˜ì„¸ìš”.
        ë‚´ê°€ ì œê³µí•œ ì¹´ë“œ ë°ì´í„°ë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ê·¸ ì•ˆì—ì„œ ì¶”ì²œí•˜ì„¸ìš”.
        í•œê¸€ë¡œ ë‹µí•˜ì§€ë§ê³  card_name, card_type, benefit, image_url ì´ëŸ°í•„ë“œë¡œ ë‹µí•˜ì„¸ìš”.
        ì´ë¯¸ì§€ ë§í¬ë„ í¬í•¨í•´ì„œ ì œê³µí•˜ì„¸ìš”. ë‹¨, ì´ë¯¸ì§€ë§í¬ëŠ” ë‚´ê°€ ê±´ë„¤ì¤€ ì¹´ë“œë°ì´í„°ì˜ image_urlì„ ê·¸ëŒ€ë¡œ í…ìŠ¤íŠ¸ë¡œ ì œê³µí•  ê²ƒ, ì´ë¯¸ì§€ ë§í¬ í•˜ì´í¼ë§í¬ ê¸ˆì§€, urlí…ìŠ¤íŠ¸ ìì²´ë¥¼ ë°˜í™˜
        ìµœì¢…ì‘ë‹µì„ í”„ëŸ°íŠ¸ì—ì„œ ì‚¬ìš©ê°€ëŠ¥í•œ jsonê°ì²´ë¡œ ë°˜í™˜í•  ê²ƒ, ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì(\n, \") ì—†ì´ ìˆœìˆ˜ JSON í˜•íƒœë¡œ ì‘ë‹µí•´ì¤˜.
        """
    )
    
    def convert_card_df_to_text(df):
        card_texts = []
        for _, row in df.iterrows():
            card_texts.append(
                f"ì¹´ë“œ ì´ë¦„: {row.get('card_name', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
                f"ì¹´ë“œ íƒ€ì…: {row.get('card_type', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
                f"í˜œíƒ: {row.get('benefits', 'ì—†ìŒ')}\n"
                f"ì´ë¯¸ì§€: {row.get('image_url', 'ì—†ìŒ')}\n"
            )
        return "\n\n".join(card_texts)
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=api_key,
        temperature=0.7,
        top_p=0.9
    )
    card_list_text = convert_card_df_to_text(all_cards)

    prompt = prompt.format(user_profile=user_profile, all_cards=card_list_text)
    response = llm.invoke([HumanMessage(content=prompt)])
    parsed_json = json.loads(response.content)
    print(parsed_json)
    final_json = {
        "credit_cards": parsed_json.get("credit_cards", []),
        "check_cards": parsed_json.get("check_cards", [])
    }

    return JSONResponse(content={
        "user_profile": user_profile,
        "recommendation": final_json 
    }, media_type="application/json")


if __name__ == "__main__":
    logger.info("ğŸš€ FastAPI ì„œë²„ ì‹¤í–‰ ì¤‘...")
    uvicorn.run(app, host="0.0.0.0", port=8100)